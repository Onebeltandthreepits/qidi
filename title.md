# 逻辑回归和线性回归区别

1）线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。 
2）线性回归要求因变量是连续性数值变量，而logistic回归要求因变量是分类型变量。 
3）线性回归要求自变量和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系 
4）logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系



# 什么是梯度下降，梯度下降优化

梯度下降法是一个最优化算法，通常也称为最速下降法.最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。

常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。

### 三种梯度下降优化框架

全量梯度下降:

~~~
for i in range(epochs):
    params_grad = evaluate_gradient(loss_function,data,params)
    params = params - learning_rate * params_grad
~~~

epochs 是用户输入的最大迭代次数。通过上诉代码可以看出，每次使用全部训练集样本计算损失函数loss_function的梯度params_grad，然后使用学习速率learning_rate朝着梯度相反方向去更新模型的每个参数params.

 全量梯度下降每次学习都使用整个训练集，因此其优点在于每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)，但是其缺点在于每次学习时间过长，并且如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。

随机梯度下降:

~~~
for i in range(epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function,example,params)
        params = params - learning_rate * params_grad
~~~

随机梯度下降最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动)。随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。

小批量梯度下降:

~~~
for i in range(epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size=50):
        params_grad = evaluate_gradient(loss_function,batch,params)
        params = params - learning_rate * params_grad
~~~

相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。

### 梯度下降优化算法

### Momentum

### NAG:

Nesterov accelerated gradient(NAG,涅斯捷罗夫梯度加速)不仅增加了动量项，并且在计算参数的梯度时，在损失函数中减去了动量项，即计算∇θJ(θ−γνt−1)∇θJ(θ−γνt−1)，这种方式预估了下一次参数所在的位置

### Adagrad:

Adagrad也是一种基于梯度的优化算法，它能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，因此该优化算法适合处理稀疏特征数据。

### RMSprop:

RMSprop是Adadelta的中间形式，也是为了降低Adagrad中学习速率衰减过快问题

### Adam:

Adaptive Moment Estimation(Adam) 也是一种不同参数自适应不同学习速率方法，与Adadelta与RMSprop区别在于，它计算历史梯度衰减方式不同，不使用历史平方衰减，其衰减方式类似动量.



# 防止过拟合以及解决过拟合

究其原因,产生过拟合是因为： 

1.由于对样本数据,可能存在隐单元的表示不唯一,即产生的分类的决策面不唯一.随着学习的进行, BP算法使权值可能收敛过于复杂的决策面,并至极致.

 2.权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.

过度拟合解决方法
1.权值衰减.
在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏
2.适当的stopping criterion
3.验证数据
  一个最成功的方法是在训练数据外再为算法提供一套验证数据,应该使用在验证集合上产生最小误差的迭代次数,不是总能明显地确定验证集合何时达到最小误差.Typically 30% of training patterns;Validation set error is checked each epoch;Stop training if validation error goes up
4.Cross-validation with some patterns
交叉验证方法在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重
k-fold交叉方法:
把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例;每次实验中,使用上面讨论的交叉验证过程来决定在验证集合上取得最佳性能的迭代次数n*,然后计算这些迭代次数的均值,作为最终需要的迭代次数。

5. 减少特征
  人工选择，预留一些特征
  利用算法选取一些比较好的特征

6. 正则化

7. 重新清洗数据

8. 增大数据的训练量

9. 采用dropout方法

   # 欠拟合解决方法

   1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。

   2）添加多项式特征。

   3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。

   4 )  使用非线性模型

   5 )  调整模型的容量